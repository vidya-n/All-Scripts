{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#armstrong check\n",
    "i = input('Give any number to check if it is an Armstrong: ')\n",
    "n = len(i)\n",
    "sum = 0\n",
    "\n",
    "for digit in i:\n",
    "    sum += int(digit) ** n\n",
    "\n",
    "if sum == int(i):\n",
    "    print(f\"{i} is an Armstrong number.\")\n",
    "else:\n",
    "    print(f\"{i} is not an Armstrong number.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.arh.org/providers/?jet-smart-filters=epro-posts/default&_tax_query_post_city=96\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "#print(response.status_code)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Extract the desired data from the parsed HTML\n",
    "    # Example: Extract all the links on the page\n",
    "    links = [link.get(\"href\") for link in soup.find_all(\"a\", class_=\"elementor-button-link elementor-button elementor-size-md\")]\n",
    "    print(links)\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to retrieve data from the URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.jacksonparkhospital.org/directory/find-a-physician/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "#print(response.status_code)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Extract the desired data from the parsed HTML\n",
    "    # Example: Extract all the links on the page\n",
    "    links = soup.find_all(\"a\", class_=\"PhysiciansDetails\")\n",
    "    print(links)\n",
    "    \n",
    "    # Print the extracted data\n",
    "    for link in links:\n",
    "        print(link.get(\"href\"))\n",
    "else:\n",
    "    print(\"Failed to retrieve data from the URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afefb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.jacksonparkhospital.org/directory/find-a-physician/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "#print(response.status_code)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the provider information\n",
    "    providers = []\n",
    "    provider_elements = soup.find_all(\"div\", class_=\"provider\")\n",
    "    \n",
    "    for provider_element in provider_elements:\n",
    "        provider = {}\n",
    "        \n",
    "        # Extract the name\n",
    "        name_element = provider_element.find(\"h3\", class_=\"provider-name\")\n",
    "        provider[\"Name\"] = name_element.text.strip()\n",
    "        \n",
    "        # Extract the profile link\n",
    "        profile_link_element = provider_element.find(\"a\", class_=\"provider-link\")\n",
    "        provider[\"Profile Link\"] = profile_link_element[\"href\"]\n",
    "        \n",
    "        # Extract the specialty\n",
    "        specialty_element = provider_element.find(\"div\", class_=\"provider-specialty\")\n",
    "        provider[\"Specialty\"] = specialty_element.text.strip()\n",
    "        \n",
    "        # Extract the address\n",
    "        address_element = provider_element.find(\"div\", class_=\"provider-address\")\n",
    "        provider[\"Address\"] = address_element.text.strip()\n",
    "        \n",
    "        # Extract the email\n",
    "        email_element = provider_element.find(\"div\", class_=\"provider-email\")\n",
    "        provider[\"Email\"] = email_element.text.strip()\n",
    "        \n",
    "        # Extract the phone\n",
    "        phone_element = provider_element.find(\"div\", class_=\"provider-phone\")\n",
    "        provider[\"Phone\"] = phone_element.text.strip()\n",
    "        \n",
    "        providers.append(provider)\n",
    "    \n",
    "    # Print the extracted datav\n",
    "    for provider in providers:\n",
    "        print(provider)\n",
    "else:\n",
    "    print(\"Failed to retrieve data from the URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d095ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all the doctor links with pagination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting all the doctor urls form the search page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aafe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#domain = \"https://doctors.summithealthcare.net/find_a_doctor/\"\n",
    "\n",
    "doctor_names = []\n",
    "doctor_urls = []\n",
    "for i in range(1, 104):\n",
    "    url = 'https://doctors.valleyhealth.com/search?sort=networks%2Crelevance%2Cavailability_density_best&page=1'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    doctors = soup.find_all('div', {'data-view': 'consumer-provider'})\n",
    "    #print(doctors)\n",
    "\n",
    "    for doctor in doctors:\n",
    "        for link in doctor.find_all(\"a\"):\n",
    "            print(link.text.strip()) # print the text of the link\n",
    "            print(link['href'])     # print the href of the link\n",
    "    \n",
    "print(f\"Page {i} done\")\n",
    "\n",
    "#print(len(doctor_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"doctorlinks = []\n",
    "\n",
    "for doctor in doctors:\n",
    "    for link in doctor.find_all('a', href=True):\n",
    "        print(link['href'])\"\"\"\n",
    "        \n",
    "\"\"\"    print(f\"Page {i} done\")\n",
    "for i in range(1, 104):\n",
    "    r = requests.get(f'https://doctors.valleyhealth.com/search?sort=networks%2Crelevance%2Cavailability_density_best&page={i}')\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    doctors = soup.find_all('div', {'data-name': 'entity_field_post_title'})\n",
    "    for doctor in doctors:\n",
    "        for link in doctor.find_all('a', href=True):\n",
    "            print(link.text.strip()) # print the text of the link\n",
    "            print(link['href'])     # print the href of the link\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0831e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doctor_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c0706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "domain  = \"https://www.adventhealth.com\"\n",
    "doctor_urls = []\n",
    "\n",
    "for i in range(1, 27):\n",
    "    search = \"https://www.adventhealth.com/hospital/adventhealth-wesley-chapel/find-doctors\"\n",
    "\n",
    "    response = requests.get(search)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    #print(soup.prettify())\n",
    "\n",
    "    doctors = soup.find_all('div', {'class': 'physician-block__cta'})\n",
    "    #print(len(doctors))\n",
    "    \n",
    "\n",
    "    for doctor in doctors:\n",
    "        for link in doctor.find_all('a', href=True):\n",
    "            doctor_urls.append(domain + link['href'])\n",
    "    #print(f\"Page {i} done\")\n",
    "print(len(doctor_urls))\n",
    "print(doctor_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37077146",
   "metadata": {},
   "outputs": [],
   "source": [
    "testlink = doctor_urls[0]\n",
    "\n",
    "r = requests.get(testlink)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "name = soup.find('h1', {'class': 'physician-details-block__name'}).text.strip()\n",
    "specialty = soup.find_all('p', {'class': 'physician-details-block__specialty'})\n",
    "address = {}\n",
    "practice_name = soup.find_all('h3', {'class': 'physician-locations-block__practice-name'})\n",
    "addLine1 = soup.find_all('span', {'property': 'streetAddress'})\n",
    "for i in addLine1:\n",
    "    print(i.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "testlink = doctor_urls[0]\n",
    "\n",
    "r = requests.get(testlink) \n",
    "tree = html.fromstring(r.content)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "name = soup.find('h1', {'class': 'physician-details-block__name'}).text.strip() \n",
    "specialty = soup.find_all('p', {'class': 'physician-details-block__specialty'}) \n",
    "address = {} \n",
    "add_count = len(tree.xpath('//div[contains(@class, \"physician-locations-block__locations-item\")]'))\n",
    "\n",
    "practice_name = tree.xpath('//div[contains(@class, \"__locations-item\")]//div[contains(@class, \"practice-name\")]/text()')\n",
    "addresses = tree.xpath('//div[contains(@class, \"physician-locations-block__practice-name\")]/following-sibling::div[1]//span[contains(@property, \"streetAddress\")]/text()')\n",
    "city = list(soup.find_all('span', {'property': 'addressLocality'}))\n",
    "state = list(soup.find_all('span', {'property': 'addressRegion'}))\n",
    "zip = list(soup.find_all('span', {'property': 'postalCode'}))\n",
    "phone = tree.xpath('//div[contains(@class, \"physician-locations-block__telephone\")]/a/span/following-sibling::text()')\n",
    "\n",
    "\n",
    "\"\"\"for i in range(add_count):\n",
    "    address[i] = {\n",
    "        'practice_name': practice_name[i].text.strip(),\n",
    "        'address': addresses[i],\n",
    "        'city': city[i].text.strip(),\n",
    "        'state': state[i].text.strip(),\n",
    "        'zip': zip[i].text.strip(),\n",
    "        'phone': phone[i]\n",
    "    }\"\"\"\n",
    "\n",
    "print(practice_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a4546",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = {\"a\": 1, \"c\": 3, \"b\": 3, \"e\": 1, \"d\": 2, \"g\": 2, \n",
    "         \"f\": 4, \"i\": 1, \"h\": 4, \"k\": 5, \"j\": 8, \"m\": 3, \n",
    "         \"l\": 1, \"o\": 1, \"n\": 1, \"q\": 10, \"p\": 3, \"s\": 1, \n",
    "         \"r\": 1, \"u\": 1, \"t\": 1, \"w\": 4, \"v\": 4, \"y\": 4, \n",
    "         \"x\": 8, \"z\": 10}\n",
    "\n",
    "def scrabble_score(word):\n",
    "    wordlow = str(word.lower())\n",
    "    score1 = 0\n",
    "    leng = len(wordlow)\n",
    "    for i in range(0, leng):\n",
    "        score1 += score[wordlow[i]]\n",
    "    print(f\"Scrabble score of \\\"{word}\\\" is {score1}.\")\n",
    "    return score1\n",
    "\n",
    "inp = input(\"Enter the word to check its scrabble score: \")\n",
    "print(\"\\n\")\n",
    "scrabble_score(inp)\n",
    "p = input(\"\\n\\nEnter any key to exit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0818e4ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WebDriver.__init__() got an unexpected keyword argument 'executable_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m options \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChromeOptions()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Then pass it to the webdriver\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchrome_driver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://www.python.org\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: WebDriver.__init__() got an unexpected keyword argument 'executable_path'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "chrome_driver = \"C:\\\\Users\\\\nyaya\\\\All Scripts\\\\chromedriver-win64\\\\chromedriver.exe\"\n",
    "\n",
    "# Create an instance of Options\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# Then pass it to the webdriver\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver, options=options)\n",
    "driver.get(\"http://www.python.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0d91652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "chrome_driver = \"C:\\\\Users\\\\nyaya\\\\All Scripts\\\\chromedriver-win64\\\\chromedriver.exe\"\n",
    "\n",
    "# Create a Service object\n",
    "s=Service(chrome_driver)\n",
    "\n",
    "# Create an instance of Options\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# Then pass it to the webdriver\n",
    "driver = webdriver.Chrome(service=s, options=options)\n",
    "driver.get(\"http://www.python.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d1559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
