{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#armstrong check\n",
    "i = input('Give any number to check if it is an Armstrong: ')\n",
    "n = len(i)\n",
    "sum = 0\n",
    "\n",
    "for digit in i:\n",
    "    sum += int(digit) ** n\n",
    "\n",
    "if sum == int(i):\n",
    "    print(f\"{i} is an Armstrong number.\")\n",
    "else:\n",
    "    print(f\"{i} is not an Armstrong number.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking scrabble score\n",
    "score = {\"a\": 1, \"c\": 3, \"b\": 3, \"e\": 1, \"d\": 2, \"g\": 2, \n",
    "         \"f\": 4, \"i\": 1, \"h\": 4, \"k\": 5, \"j\": 8, \"m\": 3, \n",
    "         \"l\": 1, \"o\": 1, \"n\": 1, \"q\": 10, \"p\": 3, \"s\": 1, \n",
    "         \"r\": 1, \"u\": 1, \"t\": 1, \"w\": 4, \"v\": 4, \"y\": 4, \n",
    "         \"x\": 8, \"z\": 10}\n",
    "\n",
    "def scrabble_score(word):\n",
    "    wordlow = str(word.lower())\n",
    "    score1 = 0\n",
    "    leng = len(wordlow)\n",
    "    for i in range(0, leng):\n",
    "        score1 += score[wordlow[i]]\n",
    "    print(f\"Scrabble score of \\\"{word}\\\" is {score1}.\")\n",
    "    return score1\n",
    "\n",
    "inp = input(\"Enter the word to check its scrabble score: \")\n",
    "print(\"\\n\")\n",
    "scrabble_score(inp)\n",
    "p = input(\"\\n\\nEnter any key to exit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doctor urls for single page\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.arh.org/providers/?jet-smart-filters=epro-posts/default&_tax_query_post_city=96\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "#print(response.status_code)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Extract the desired data from the parsed HTML\n",
    "    # Example: Extract all the links on the page\n",
    "    links = [link.get(\"href\") for link in soup.find_all(\"a\", class_=\"elementor-button-link elementor-button elementor-size-md\")]\n",
    "    print(links)\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to retrieve data from the URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfinished - extracting links from all pages.\n",
    "#To Do - Resolve iframe issue\n",
    "\n",
    "# Doctor urls for multiple pages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#domain = \"https://doctors.summithealthcare.net/find_a_doctor/\"\n",
    "\n",
    "doctor_names = []\n",
    "doctor_urls = []\n",
    "for i in range(1, 104):\n",
    "    url = 'https://doctors.valleyhealth.com/search?sort=networks%2Crelevance%2Cavailability_density_best&page=1'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    doctors = soup.find_all('h2', {'class': 'ProviderName'})\n",
    "    #print(doctors)\n",
    "\n",
    "    for doctor in doctors:\n",
    "        for link in doctor.find_all(\"a\"):\n",
    "            print(link.text.strip()) # print the text of the link\n",
    "            print(link['href'])     # print the href of the link\n",
    "    \n",
    "print(f\"{i} pages done\")\n",
    "#print(doctor_urls)\n",
    "#print(len(doctor_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract doctor urls from pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "domain  = \"https://www.adventhealth.com\"\n",
    "doctor_urls = []\n",
    "\n",
    "for i in range(1, 27):\n",
    "    search = \"https://www.adventhealth.com/hospital/adventhealth-wesley-chapel/find-doctors\"\n",
    "\n",
    "    response = requests.get(search)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    #print(soup.prettify())\n",
    "\n",
    "    doctors = soup.find_all('div', {'class': 'physician-block__cta'})\n",
    "    #print(len(doctors))\n",
    "    \n",
    "\n",
    "    for doctor in doctors:\n",
    "        for link in doctor.find_all('a', href=True):\n",
    "            doctor_urls.append(domain + link['href'])\n",
    "    #print(f\"Page {i} done\")\n",
    "print(len(doctor_urls))\n",
    "print(doctor_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Getting Details of 1 doctor url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "testlink = 'https://www.adventhealth.com/hospital/adventhealth-wesley-chapel/find-doctor/doctor/olufunke-abiose-md-1538360680'\n",
    "\n",
    "r = requests.get(testlink) \n",
    "tree = html.fromstring(r.content)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "name = soup.find('h1', {'class': 'physician-details-block__name'}).text.strip() \n",
    "specialty_block = soup.find_all('p', {'class': 'physician-details-block__specialty'})\n",
    "specialty = [spec.text.strip() for spec in specialty_block]\n",
    "address = {} \n",
    "add_count = len(tree.xpath('//div[contains(@class, \"physician-locations-block__locations-item\")]'))\n",
    "\n",
    "practice_name = tree.xpath('//div[contains(@class, \"__locations-item\")]//div[contains(@class, \"practice-name\")]/text()')\n",
    "addresses = tree.xpath('//div[contains(@class, \"physician-locations-block__practice-name\")]/following-sibling::div[1]//span[contains(@property, \"streetAddress\")]/text()')\n",
    "city = list(soup.find_all('span', {'property': 'addressLocality'}))\n",
    "state = list(soup.find_all('span', {'property': 'addressRegion'}))\n",
    "zip = list(soup.find_all('span', {'property': 'postalCode'}))\n",
    "phone = tree.xpath('//div[contains(@class, \"physician-locations-block__telephone\")]/a/span/following-sibling::text()')\n",
    "\n",
    "for i in range(0, add_count):\n",
    "    address[i] = {'practice_name': practice_name[i], 'address': addresses[i], 'city': city[i].text, 'state': state[i].text, 'zip': zip[i].text, 'phone': phone[i]}\n",
    "\n",
    "doctor_details = {'name': name, 'specialty': specialty, 'address': address}\n",
    "print(doctor_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Getting details of all doctor urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "domain  = \"https://www.adventhealth.com\"\n",
    "doctor_urls = []\n",
    "\n",
    "for i in range(1, 27):\n",
    "    search = \"https://www.adventhealth.com/hospital/adventhealth-wesley-chapel/find-doctors\"\n",
    "\n",
    "    response = requests.get(search)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    #print(soup.prettify())\n",
    "\n",
    "    doctors = soup.find_all('div', {'class': 'physician-block__cta'})\n",
    "    #print(len(doctors))\n",
    "    \n",
    "\n",
    "    for doctor in doctors:\n",
    "        for link in doctor.find_all('a', href=True):\n",
    "            doctor_urls.append(domain + link['href'])\n",
    "    #print(f\"Page {i} done\")\n",
    "\"\"\"print(len(doctor_urls))\n",
    "print(doctor_urls)\"\"\"\n",
    "doc_count = 0\n",
    "\n",
    "for j in doctor_urls:\n",
    "    r = requests.get(j) \n",
    "    tree = html.fromstring(r.content)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    name = soup.find('h1', {'class': 'physician-details-block__name'}).text.strip() \n",
    "    specialty_block = soup.find_all('p', {'class': 'physician-details-block__specialty'})\n",
    "    specialty = [spec.text.strip() for spec in specialty_block]\n",
    "    address = {} \n",
    "    add_count = len(tree.xpath('//div[contains(@class, \"physician-locations-block__locations-item\")]'))\n",
    "\n",
    "    practice_name = tree.xpath('//div[contains(@class, \"__locations-item\")]//div[contains(@class, \"practice-name\")]/text()')\n",
    "    addresses = tree.xpath('//div[contains(@class, \"physician-locations-block__practice-name\")]/following-sibling::div[1]//span[contains(@property, \"streetAddress\")]/text()')\n",
    "    city = list(soup.find_all('span', {'property': 'addressLocality'}))\n",
    "    state = list(soup.find_all('span', {'property': 'addressRegion'}))\n",
    "    zip = list(soup.find_all('span', {'property': 'postalCode'}))\n",
    "    phone = tree.xpath('//div[contains(@class, \"physician-locations-block__telephone\")]/a/span/following-sibling::text()')\n",
    "\n",
    "    for i in range(0, add_count):\n",
    "        address[i] = {'practice_name': practice_name[i], 'address': addresses[i], 'city': city[i].text, 'state': state[i].text, 'zip': zip[i].text, 'phone': phone[i]}\n",
    "\n",
    "    doctor_details = {'name': name, 'specialty': specialty, 'address': address}\n",
    "    doc_count += 1\n",
    "    print(doc_count)\n",
    "\n",
    "print(\"Scraping done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load more button page type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doctor urls for 1st page.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#domain  = \"https://www.adventhealth.com\"\n",
    "doctor_urls = []\n",
    "\n",
    "\"\"\"for i in range(1, 27):\"\"\"\n",
    "search = \"https://doctors.summithealthcare.net/find_a_doctor/locations/show_low/\"\n",
    "\n",
    "response = requests.get(search)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "#print(soup.prettify())\n",
    "\n",
    "doctors = soup.find_all('div', {'data-name': 'entity_field_post_title'})\n",
    "for doctor in doctors:\n",
    "    for link in doctor.find_all('a', href=True):\n",
    "        doctor_urls.append(link['href'])\n",
    "\n",
    "print(len(doctor_urls))\n",
    "print(doctor_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://doctors.summithealthcare.net/find_a_doctor/locations/show_low/?_page=1&num=20&sort=field_physician_name%2Clast_name\n",
      "https://doctors.summithealthcare.net/find_a_doctor/locations/show_low/?_page=2&num=20&sort=field_physician_name%2Clast_name\n",
      "https://doctors.summithealthcare.net/find_a_doctor/locations/show_low/?_page=3&num=20&sort=field_physician_name%2Clast_name\n",
      "https://doctors.summithealthcare.net/find_a_doctor/locations/show_low/?_page=4&num=20&sort=field_physician_name%2Clast_name\n",
      "https://doctors.summithealthcare.net/find_a_doctor/locations/show_low/?_page=5&num=20&sort=field_physician_name%2Clast_name\n",
      "https://doctors.summithealthcare.net/find_a_doctor/locations/show_low/?_page=6&num=20&sort=field_physician_name%2Clast_name\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "# Doctor urls with load more - all doctors\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL from the AJAX request when clicking \"Load More\"\n",
    "url = \"https://doctors.summithealthcare.net/find_a_doctor/locations/show_low/\"\n",
    "\n",
    "# These headers and params are just examples, you need to get them from the actual request\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "#params = {\"page\": 3}  # This might be how the website keeps track of pagination\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "#print(soup.prettify())\n",
    "\n",
    "doctor_urls = []\n",
    "page_count = 6\n",
    "sum = 20\n",
    "load_more_a = soup.find('a', {'class': 'drts-view-nav-item-name-load_more'})\n",
    "load_more = load_more_a['href']\n",
    "for i in range(1, page_count+1):\n",
    "    modified_href = load_more.replace(\"page=2\", f\"page={i}\")\n",
    "    print(modified_href)\n",
    "    response = requests.get(modified_href, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    doctors = soup.find_all('div', {'data-name': 'entity_field_post_title'})\n",
    "    for doctor in doctors:\n",
    "        for link in doctor.find_all('a', href=True):\n",
    "            doctor_urls.append(link['href'])\n",
    "\n",
    "\n",
    "print(len(doctor_urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
